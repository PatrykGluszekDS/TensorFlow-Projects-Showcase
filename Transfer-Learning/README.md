# AgriAid - Plant-Disease Diagnosis ðŸŒ¿ðŸ“¸

**Plant Disease Diagnosis with Transfer Learning (EfficientNet, ResNet)**

> Identify 38 cropâ€‘disease categories from 54â€¯k+ leaf images, package the best model for onâ€‘device inference.

## Project Highlights

* ðŸ” **Transfer Learning**: EfficientNet, ResNet50
* ðŸ§ª **Rigorous Experiments**: baseline, fineâ€‘tuning, loss functions, dataâ€‘augmentation.
* ðŸŒ± **Impact**: Helps farmers diagnose diseases early.

## Dataset

* **PlantVillage** (`plant_village` in TFDS) â€” 54â€¯303 RGB images, 256â€¯Ã—â€¯256â€¯px, 38 classes (healthy + diseased leaves). ([tensorflow.org](https://www.tensorflow.org/datasets/catalog/plant_village))
* Predefined `train`/`validation`/`test` splits created inside the notebook for reproducibility.

## Exploratory Data Analysis

> **RAMâ€‘friendly tip:** If Colab runs out of memory, skip the inâ€‘memory stratified split and use TFDS percentage subsplits instead (e.g. `tfds.load('plant_village', split=['train[:80%]','train[80%:90%]','train[90%:]'], as_supervised=True)`). This streams images from disk and keeps peak RAM low.

* **Split sizes:** 43â€¯442 (train) Â· 5â€¯431 (val) Â· 5â€¯430 (test) â€” checked with `tf.data.experimental.cardinality(...)`.

## Data Preprocessing

All three splits are piped through the same lightweight preprocessing function before entering the model:

```python
@tf.function
def preprocess_img(image, label, img_shape=224):
    """Resizes and casts the PlantVillage image to float32."""
    image = tf.image.resize(image, [img_shape, img_shape])          # 256Ã—256 â†’ 224Ã—224
    # NOTE: EfficientNet/ResNet builtâ€‘in preprocessing includes scaling; additional /255 not required.
    return tf.cast(image, tf.float32), label

# Apply, shuffle, batch, prefetch
BATCH_SIZE = 32
AUTOTUNE   = tf.data.AUTOTUNE

ds_train = (ds_train
            .map(preprocess_img, num_parallel_calls=AUTOTUNE)
            .shuffle(1000, seed=SEED)
            .batch(BATCH_SIZE)
            .prefetch(AUTOTUNE))

ds_val = (ds_val
          .map(preprocess_img, num_parallel_calls=AUTOTUNE)
          .batch(BATCH_SIZE)
          .prefetch(AUTOTUNE))

ds_test = (ds_test
           .map(preprocess_img, num_parallel_calls=AUTOTUNE)
           .batch(BATCH_SIZE)
           .prefetch(AUTOTUNE))
```

**Why this matters**

* Resizing early keeps GPU utilisation high and avoids onâ€‘device resize overhead.
* `.shuffle â†’ .batch â†’ .prefetch` ensures an efficient input pipeline that overlaps data loading with model execution.

## Getting Started

1. **Open in Colab** â†’ `Runtime â†’ Change runtime type â†’ GPU`.
2. Run the notebook.

## Experiments & Results

| #  | Backbone & Variant                       | Training strategy               | Input px | Valâ€¯Acc     | Testâ€¯Acc    | Notes                                             |
| -- | ---------------------------------------- | ------------------------------- | -------- | ----------- | ----------- | ------------------------------------------------- |
| 1  | EfficientNetV2â€‘B0                        | frozen feature extractor        | 224      | **0.982**   | **0.984**   | Mixedâ€‘precision; 5â€¯epochs; ES + RLRP              |
| 2  | EfficientNetV2â€‘B0 + **Aug**              | frozen extractor + inâ€‘model aug | 224      | 0.954       | 0.960       | Flip, rotateâ€¯Â±0.2, zoom/height/widthâ€¯0.2          |
| 3  | EfficientNetV2â€‘B0 + **Aug + FT30**       | fineâ€‘tune last 30 layers        | 224      | 0.968       | 0.971       | 35â€¯epochs total; lr 1eâ€‘4â†’plateau                  |
| 4  | ResNet50 (**wrong preprocessing**)       | frozen feature extractor        | 224      | 0.407       | 0.430       | Used only Rescaling 1/255 â†’ mismatch              |
| 5  | **ResNet50 (official preprocess)**       | frozen feature extractor        | 224      | **0.982**   | **0.985**   | `resnet.preprocess_input` (BGR + means), 5â€¯epochs |
| 6  | ResNet50 + **Aug** (official preprocess) | frozen extractor + inâ€‘model aug | 224      | 0.946       | 0.949       | Flip/rotate/zoom/height/widthâ€¯0.2                 |
| 7  | **ResNet50 + Aug + FT30**                | fineâ€‘tune last 30 layers        | 224      | **â‰ˆâ€¯0.989** | **â‰ˆâ€¯0.990** | 35â€¯epochs total; RLRP stepped LR; ES enabled      |

**Expâ€‘2 summary (EfficientNetV2â€‘B0â€¯+â€¯Aug)**

* **Data augmentation**: `RandomFlip('horizontal')`, `RandomRotation(0.2)`, `RandomHeight(0.2)`, `RandomWidth(0.2)`, `RandomZoom(0.2)`.
* Backbone **frozen**, mixedâ€‘precision enabled.
* Adam, lrâ€¯=â€¯1â€¯eâ€‘3; callbacks: EarlyStopping, ReduceLROnPlateau.
* **5 epochs** â†’ **Valâ€¯0.954 / Testâ€¯0.960**, lossâ€¯â‰ˆâ€¯0.13.

---

**Expâ€‘3 summary (EfficientNetV2â€‘B0â€¯+â€¯Aug, fineâ€‘tuned 30 layers)**

* Starting from Expâ€‘2 weights, **unfroze last 30 layers** of the backbone (`base_model_2.layers[-30:]`).
* Reâ€‘compiled with Adam, lrâ€¯=â€¯4â€¯eâ€‘5.
* Trained to **epochâ€¯25/35** with EarlyStoppingâ€¯+â€¯ReduceLROnPlateau.
* Final metrics: **Valâ€¯â‰ˆâ€¯0.968**, **Testâ€¯=â€¯0.971**, lossâ€¯â‰ˆâ€¯0.088.

---

**Expâ€‘4 summary (ResNet50 baseline â€“ wrong preprocessing)**

* Featureâ€‘extraction pass with **`tf.keras.applications.ResNet50`** (`include_top=False`, `weights='imagenet'`).
* Used only `layers.Rescaling(1./255)` â†’ **no ImageNet mean subtraction / RGBâ†”BGR swap** â†’ distribution mismatch.
* Mixedâ€‘precision on; Adam lrâ€¯=â€¯1eâ€‘3; 5â€¯epochs.
* Results: **Val â‰ˆ 0.407**, **Test â‰ˆ 0.430**, loss â‰ˆ 2.22.
* Conclusion: preprocessing mismatch severely hurts performance; fix with `resnet.preprocess_input` or switch to ResNet50V2 + \[-1,1] scaling.

---

**Expâ€‘5 summary (ResNet50 with official preprocessing)**

* Rebuilt baseline with **`from tensorflow.keras.applications.resnet import preprocess_input`** ahead of the backbone (does **BGR conversion** + **channelâ€‘wise mean subtraction**).
* Backbone **frozen**; no augmentation; mixedâ€‘precision; Adam lrâ€¯=â€¯1â€¯eâ€‘3; **5 epochs**.
* Final metrics: **Val â‰ˆ 0.982**, **Test â‰ˆ 0.986**, loss â‰ˆ 0.045.
* Takeaway: correct preprocessing restores expected performance; now competitive with EfficientNetV2â€‘B0.

---

**Expâ€‘6 summary (ResNet50 + Aug, official preprocessing)**

* Same official preprocessing as Expâ€‘5; **inâ€‘model augmentation**: `RandomFlip('horizontal')`, `RandomRotation(0.2)`, `RandomHeight(0.2)`, `RandomWidth(0.2)`, `RandomZoom(0.2)`.
* Backbone **frozen**; 5â€¯epochs; mixedâ€‘precision; Adam lrâ€¯=â€¯1eâ€‘3.
* Final metrics: **Val â‰ˆ 0.946**, **Test â‰ˆ 0.949**, loss â‰ˆ 0.160.
* Interpretation: strong aug with a frozen backbone reduces accuracy vs noâ€‘aug baseline. Options: (1) lighten aug (rotation/zoom/height/width â†’ 0.1), (2) train longer, or (3) **fineâ€‘tune the last 30â€“40 layers** at lrâ€¯â‰ˆâ€¯1eâ€‘4 with the same aug.

**Expâ€‘7 summary (ResNet50 + augmentation, fineâ€‘tuned last 30 layers)**

* Continued from Expâ€‘6: set `base_model_6.trainable=True`, then **reâ€‘froze all but the last 30 layers**.
* Same official `resnet.preprocess_input`; kept inâ€‘model augmentation.
* Optimizer Adam; **ReduceLROnPlateau** automatically reduced LR (2eâ€‘4 â†’ 4eâ€‘5 â†’ 8eâ€‘6, etc.).
* Trained to epoch \~16/35 (EarlyStopping active). Final metrics: **Val â‰ˆ 0.989**, **Test â‰ˆ 0.990**, loss â‰ˆ 0.034.
* Takeaway: fineâ€‘tuning with moderate aug surpasses all previous runs; this is a strong candidate for the model ready to export.
